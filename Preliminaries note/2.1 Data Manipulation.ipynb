{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch数据操作笔记\n",
    "\n",
    "1. tensor(张量）概念\n",
    "  张量是神经网络使用的最主要数据结构，形式上类似n维数组,张量支持CPU、GPU上的异步计算，并支持自动微分，具体属性如下：\n",
    "   - rank：需要多少个索引来访问张量种的数据元素\n",
    "   - length of axis: 每个张量轴的长度，类似数组最大长度\n",
    "   - shape: 类似m*n矩阵表示法\n",
    "2. tensor构建常用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t \n",
    "\n",
    "x= t.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arange 函数会默认产生从0开始的n个连续的整数，并把他们存在一个x的向量里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # 查看tensor的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=x.reshape((3,4)) # 重置张量的shape\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= x.reshape((2,-1)) # 当张量rank为2时，只要输入一个维度指定长度，另一个可以用-1替换\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1456e-07, 1.0027e-11, 5.3772e+22],\n",
       "        [2.1629e+23, 1.7565e-04, 6.3013e-10]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.empty(2,3) # 随机获取内存的数值构建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(3,3) # 初始化张量全0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2,3,4) # 初始化张量全1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 3., 4.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.Tensor([[1,2,3],[2,3,4]]) # 将指定python列表转换为张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7161,  0.3105, -0.2103],\n",
       "        [ 0.2853,  0.5238, -1.7068]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2,3) # 获得的随机张量数值是标准正态分布的可能取值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. tensor常用计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10., 10.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.Tensor([1,2,3])\n",
    "y = t.ones_like(x)*10  # 初始化和x一样张量，*n会把所有元素都变成n\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([11., 12., 13.]),\n",
       " tensor([10., 20., 30.]),\n",
       " tensor([-9., -8., -7.]),\n",
       " tensor([0.1000, 0.2000, 0.3000]),\n",
       " tensor([1.0000e+00, 1.0240e+03, 5.9049e+04]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y,x*y,x-y,x/y,x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7183,  7.3891, 20.0855])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.exp(x) # 对每个元素进行e求幂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 4., 5.],\n",
       "         [4., 5., 6.]]),\n",
       " tensor([[2., 4., 6.],\n",
       "         [4., 6., 8.]]),\n",
       " tensor([[-1.,  0.,  1.],\n",
       "         [ 0.,  1.,  2.]]),\n",
       " tensor([[0.5000, 1.0000, 1.5000],\n",
       "         [1.0000, 1.5000, 2.0000]]),\n",
       " tensor([[ 1.,  4.,  9.],\n",
       "         [ 4.,  9., 16.]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.Tensor([[1,2,3],[2,3,4]])\n",
    "y = t.ones_like(x)*2\n",
    "x+y,x*y,x-y,x/y,x**y # 这里运算跟矩阵的不一样，都是对应元素做正常加减乘除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 23.,  29.,  35.],\n",
       "        [ 67.,  89., 111.],\n",
       "        [111., 149., 187.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = t.Tensor([[1,2,3],[2,3,4],[3,4,5],[5,6,7]])\n",
    "# y.t()  #  这里是矩阵操作，求张量转置\n",
    "x= t.arange(12,dtype=torch.float32).reshape(3,-1) # dtype=torch.float32必须加上，不然会报类型不一致错误\n",
    "t.mm(x,y)  # 这里是矩阵的乘法操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  1.,  2.,  3.,  5.],\n",
       "        [ 4.,  5.,  6.,  7.,  2.,  3.,  4.,  6.],\n",
       "        [ 8.,  9., 10., 11.,  3.,  4.,  5.,  7.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cat((x,y.t()),dim=1) # 矩阵合并操作，dim指定合并维度，0按行，1按列，两个张量必须同型才能执行合并操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x==y.t() # 对同型张量的同位置的数据进行比较，返回的张量对应比较结果，相同为True,不同为False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum() # 对张量里所有元素求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.广播机制\n",
    "\n",
    "  当两个张量的形状不同时，可以使用类似NumPy的广播机制：首先适当地复制元素，使两个张量具有相同的形状，然后按元素进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 2.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3, dtype=torch.float).reshape((3, 1))\n",
    "b = torch.arange(2, dtype=torch.float).reshape((1, 2))\n",
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为a和b分别是（3x1）和（1x2）矩阵，所以如果我们要将它们相加，它们的形状就不匹配。Pythorch通过将两个矩阵的条目“广播”到一个更大的（3x2）矩阵中来解决这个问题，如上所示：对于矩阵a，它复制列，对于矩阵b，它在将两个元素相加之前复制行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 切片和索引机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= t.arange(12).reshape(3,-1)\n",
    "x[1:]   # 是按行切片的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2,2] # 对相应的下表元素操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 12, 12, 12],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:1,:]=12\n",
    "x             # 给指定行赋相同的值[0:n,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 节省内存\n",
    "\n",
    "   在前面的示例中，每次运行操作时，我们都会分配新内存来承载其结果。例如，如果我们写y=x+y，我们将取消引用y用来指向的矩阵，而将它指向新分配的内存。在下面的示例中，我们用Python的id（）函数来演示这一点，该函数给出了被引用对象在内存中的确切地址。运行y=y+x之后，我们将发现id（y）指向另一个位置。这是因为Python首先计算y+x，为结果分配新的内存，然后将y重定向到内存中的这个新位置。\n",
    "   - 定义一个相同类型的张量z,使用z[:]接收结果\n",
    "   - 使用运算的内置方法替换符号，并通过out参数指定结果指向\n",
    "   - 若x不参与后续运算，使用x+=y来直接覆盖\n",
    " \n",
    "\n",
    "7. torch和numpy转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = x.numpy()\n",
    "print(type(a))\n",
    "b = torch.tensor(a)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= t.arange(12,dtype=torch.float32).reshape(3,-1)\n",
    "y = t.Tensor([[1,2,3],[2,3,4],[3,4,5],[5,6,7]]).t()\n",
    "x<y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 24.,  30.,  36.],\n",
       "        [ 68.,  90., 112.],\n",
       "        [112., 150., 188.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = t.ones(3,3)\n",
    "id1 =id(z)\n",
    "z+=t.mm(x,y.t())\n",
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
